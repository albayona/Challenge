{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# K-Means Clustering Model (k=3) with Z-Score Standardization\n",
        "## Stock Data Clustering Analysis\n",
        "\n",
        "This notebook implements K-means clustering with k=3 clusters using Z-score standardization.\n",
        "We'll analyze the independent features dataset to identify distinct stock recommendation patterns with fewer, more distinct clusters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "from tabulate import tabulate\n",
        "from collections import Counter\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Sklearn imports\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score, silhouette_samples\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the independent features dataset\n",
        "df = pd.read_csv('stock_data_independent_features.csv')\n",
        "print(f\"Dataset shape: {df.shape}\")\n",
        "print(f\"Features: {list(df.columns)}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data quality check and cleaning\n",
        "print(\"=== DATA QUALITY CHECK ===\")\n",
        "print(f\"Infinite values: {np.isinf(df).sum().sum()}\")\n",
        "print(f\"NaN values: {df.isnull().sum().sum()}\")\n",
        "\n",
        "# Check for extremely large values\n",
        "large_values = (np.abs(df.select_dtypes(include=[np.number])) > 1e10).sum().sum()\n",
        "print(f\"Values > 1e10: {large_values}\")\n",
        "\n",
        "# Clean the data\n",
        "print(\"\\n=== CLEANING DATA ===\")\n",
        "# Replace infinite values with NaN\n",
        "df_clean = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "# Drop rows with any NaN values\n",
        "df_clean = df_clean.dropna()\n",
        "\n",
        "# Clip extremely large values to reasonable range\n",
        "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "for col in numeric_cols:\n",
        "    df_clean[col] = df_clean[col].clip(lower=-1e6, upper=1e6)\n",
        "\n",
        "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
        "print(f\"Features: {list(df_clean.columns)}\")\n",
        "\n",
        "# Verify no problematic values remain\n",
        "print(f\"\\nAfter cleaning:\")\n",
        "print(f\"Infinite values: {np.isinf(df_clean).sum().sum()}\")\n",
        "print(f\"NaN values: {df_clean.isnull().sum().sum()}\")\n",
        "print(f\"Values > 1e6: {(np.abs(df_clean.select_dtypes(include=[np.number])) > 1e6).sum().sum()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Z-Score Standardization (StandardScaler)\n",
        "print(\"=== Z-SCORE STANDARDIZATION ===\")\n",
        "print(\"Z-score formula: z = (x - μ) / σ\")\n",
        "print(\"Where:\")\n",
        "print(\"- x = original value\")\n",
        "print(\"- μ = mean of the feature\")\n",
        "print(\"- σ = standard deviation of the feature\")\n",
        "print(\"- Result: mean=0, std=1 for each feature\")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(df_clean)\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=df_clean.columns)\n",
        "\n",
        "print(f\"\\nStandardized data shape: {scaled_data.shape}\")\n",
        "print(f\"\\nZ-score statistics (should be mean≈0, std≈1):\")\n",
        "stats_summary = scaled_df.describe().round(3)\n",
        "print(stats_summary[['mean', 'std']])\n",
        "\n",
        "# Verify Z-score properties\n",
        "print(f\"\\n=== Z-SCORE VERIFICATION ===\")\n",
        "print(f\"Mean of all features: {scaled_df.mean().mean():.6f} (should be ≈0)\")\n",
        "print(f\"Std of all features: {scaled_df.std().mean():.6f} (should be ≈1)\")\n",
        "print(f\"Min value: {scaled_df.min().min():.3f}\")\n",
        "print(f\"Max value: {scaled_df.max().max():.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration parameters for 3 clusters\n",
        "K = 3  # Number of clusters (reduced from 9 to 3)\n",
        "N_INIT = 10  # Number of initializations\n",
        "SEED = 42  # Random state for reproducibility\n",
        "\n",
        "print(f\"=== CONFIGURATION ===\")\n",
        "print(f\"- Number of clusters (k): {K}\")\n",
        "print(f\"- Number of initializations: {N_INIT}\")\n",
        "print(f\"- Random seed: {SEED}\")\n",
        "print(f\"- Standardization: Z-score (mean=0, std=1)\")\n",
        "print(f\"\\nRationale for k=3:\")\n",
        "print(\"- Fewer clusters create more distinct, interpretable groups\")\n",
        "print(\"- Easier to identify clear patterns and business insights\")\n",
        "print(\"- Reduces over-segmentation of similar recommendations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to search for best seed (from Taller_4 methodology)\n",
        "def search_seed(seeds, data, k, n_init):\n",
        "    best_seed = None\n",
        "    best_score = -1\n",
        "    \n",
        "    for seed in seeds:\n",
        "        km_model = KMeans(\n",
        "            n_clusters=k, init='k-means++', random_state=seed, n_init=n_init)\n",
        "        y_predicted = km_model.fit_predict(data)\n",
        "\n",
        "        silhouette_avg = silhouette_score(data, y_predicted)\n",
        "        \n",
        "        if silhouette_avg > best_score:\n",
        "            best_score = silhouette_avg\n",
        "            best_seed = seed\n",
        "            \n",
        "        print(f\"Seed {seed}: Silhouette Score = {silhouette_avg:.4f}\")\n",
        "    \n",
        "    return best_seed, best_score\n",
        "\n",
        "# Search for best seed\n",
        "seeds_to_test = [0, 42, 123, 456, 789]\n",
        "best_seed, best_score = search_seed(seeds_to_test, scaled_data, K, N_INIT)\n",
        "\n",
        "print(f\"\\n=== BEST SEED RESULTS ===\")\n",
        "print(f\"Best seed: {best_seed}\")\n",
        "print(f\"Best silhouette score: {best_score:.4f}\")\n",
        "print(f\"\\nSilhouette Score Interpretation:\")\n",
        "print(\"- Range: -1 to +1\")\n",
        "print(\"- +1: Perfect clustering\")\n",
        "print(\"- 0: Overlapping clusters\")\n",
        "print(\"- -1: Incorrect clustering\")\n",
        "print(f\"- Our score: {best_score:.4f} ({'Good' if best_score > 0.5 else 'Fair' if best_score > 0.3 else 'Poor'})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train the final K-means model with best seed\n",
        "print(f\"Training K-means model with k={K} and seed={best_seed}...\")\n",
        "\n",
        "final_km = KMeans(n_clusters=K, init='k-means++', random_state=best_seed, n_init=N_INIT)\n",
        "final_km.fit(scaled_data)\n",
        "y_predicted = final_km.predict(scaled_data)\n",
        "\n",
        "# Calculate final metrics\n",
        "silhouette_avg = silhouette_score(scaled_data, y_predicted)\n",
        "inertia = final_km.inertia_\n",
        "\n",
        "print(f\"\\n=== MODEL RESULTS ===\")\n",
        "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
        "print(f\"Inertia: {inertia:.2f}\")\n",
        "print(f\"Number of iterations: {final_km.n_iter_}\")\n",
        "print(f\"Converged: {final_km.n_iter_ < final_km.max_iter}\")\n",
        "print(f\"\\nModel Performance:\")\n",
        "print(f\"- Silhouette Score: {'Excellent' if silhouette_avg > 0.7 else 'Good' if silhouette_avg > 0.5 else 'Fair' if silhouette_avg > 0.3 else 'Poor'}\")\n",
        "print(f\"- Convergence: {'Yes' if final_km.n_iter_ < final_km.max_iter else 'No'}\")\n",
        "print(f\"- Inertia: Lower is better (measures within-cluster sum of squares)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to print cluster centroids (from Taller_4 methodology)\n",
        "def print_clusters_centroids(model, feature_names, k):\n",
        "    info = {\n",
        "        \"features\": feature_names \n",
        "    }\n",
        "\n",
        "    for n in range(0, k):\n",
        "        info[str(n)] = model.cluster_centers_[n]\n",
        "\n",
        "    print(tabulate(info, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
        "    print(f\"\\nCluster distribution:\")\n",
        "    print(Counter(model.labels_))\n",
        "\n",
        "# Print cluster centroids and distribution\n",
        "print(\"=== CLUSTER CENTROIDS (Z-SCORE VALUES) ===\")\n",
        "print(\"Note: Values are in Z-score units (mean=0, std=1)\")\n",
        "print(\"Positive values = above average, Negative values = below average\")\n",
        "print_clusters_centroids(final_km, scaler.feature_names_in_, K)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create clusters dataframe for analysis\n",
        "clusters_df = df_clean.copy()\n",
        "clusters_df['cluster_label'] = final_km.labels_\n",
        "\n",
        "print(f\"Clusters dataframe shape: {clusters_df.shape}\")\n",
        "print(f\"\\nCluster distribution:\")\n",
        "cluster_counts = clusters_df['cluster_label'].value_counts().sort_index()\n",
        "for cluster_id, count in cluster_counts.items():\n",
        "    percentage = (count / len(clusters_df)) * 100\n",
        "    print(f\"Cluster {cluster_id}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "print(f\"\\nFirst few rows with cluster labels:\")\n",
        "clusters_df.head()\n",
        "\n",
        "# Calculate cluster balance\n",
        "max_cluster_size = cluster_counts.max()\n",
        "min_cluster_size = cluster_counts.min()\n",
        "balance_ratio = min_cluster_size / max_cluster_size\n",
        "\n",
        "print(f\"\\n=== CLUSTER BALANCE ===\")\n",
        "print(f\"Largest cluster: {max_cluster_size} samples\")\n",
        "print(f\"Smallest cluster: {min_cluster_size} samples\")\n",
        "print(f\"Balance ratio: {balance_ratio:.3f}\")\n",
        "print(f\"Balance assessment: {'Well balanced' if balance_ratio > 0.5 else 'Moderately balanced' if balance_ratio > 0.3 else 'Unbalanced'}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to plot clusters (from Taller_4 methodology)\n",
        "def plot_clusters(model, data):\n",
        "    plt.figure(figsize=(20, 10))\n",
        "\n",
        "    label = model.labels_\n",
        "    u_labels = np.unique(label)\n",
        "\n",
        "    # Getting the Centroids\n",
        "    centroids = model.cluster_centers_\n",
        "    \n",
        "    # Plotting the results:\n",
        "    for i in u_labels:\n",
        "        plt.scatter(data[label == i, 0], data[label == i, 1], \n",
        "                   label=f'Cluster {i}', alpha=0.7, s=50)\n",
        "    \n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1], \n",
        "               s=200, c='red', marker='x', label='Centroids', linewidths=3)\n",
        "    \n",
        "    plt.legend()\n",
        "    plt.title(f'K-Means Clustering (k={K}) - Z-Score Standardized')\n",
        "    plt.xlabel('First Principal Component')\n",
        "    plt.ylabel('Second Principal Component')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "# Apply PCA for 2D visualization\n",
        "pca = PCA(n_components=2)\n",
        "data_pca = pca.fit_transform(scaled_data)\n",
        "\n",
        "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
        "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
        "print(f\"Note: {pca.explained_variance_ratio_.sum():.1%} of variance explained in 2D\")\n",
        "\n",
        "# Plot clusters in 2D PCA space\n",
        "plot_clusters(final_km, data_pca)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to draw polar chart (from Taller_4 methodology)\n",
        "def draw_polar_chart(clusters):\n",
        "    polar = clusters.groupby(\"cluster_label\").mean().reset_index()\n",
        "    polar = pd.melt(polar, id_vars=[\"cluster_label\"])\n",
        "\n",
        "    fig = px.line_polar(polar, r=\"value\", theta=\"variable\", color=\"cluster_label\", \n",
        "                       line_close=True, height=800, width=1400,\n",
        "                       title=\"Cluster Characteristics - Polar Chart (Original Scale)\")\n",
        "    fig.show()\n",
        "\n",
        "# Create polar chart to visualize cluster characteristics\n",
        "print(\"Generating polar chart to visualize cluster characteristics...\")\n",
        "print(\"Note: Values shown are in original scale, not Z-scores\")\n",
        "draw_polar_chart(clusters_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to plot clusters pie chart (from Taller_4 methodology)\n",
        "def plot_clusters_pie(clusters):\n",
        "    pie = clusters.groupby('cluster_label').size().reset_index()\n",
        "    pie.columns = ['cluster_label', 'value']\n",
        "    fig = px.pie(pie, values='value', names='cluster_label', \n",
        "                title=f\"Cluster Distribution (k={K})\")\n",
        "    fig.show()\n",
        "\n",
        "# Create pie chart for cluster distribution\n",
        "print(\"Generating pie chart for cluster distribution...\")\n",
        "plot_clusters_pie(clusters_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed cluster analysis\n",
        "print(\"=== DETAILED CLUSTER ANALYSIS ===\")\n",
        "\n",
        "# Calculate mean values for each cluster (original scale)\n",
        "cluster_means = clusters_df.groupby('cluster_label').mean()\n",
        "\n",
        "print(\"\\nCluster characteristics (mean values - original scale):\")\n",
        "print(cluster_means.round(3))\n",
        "\n",
        "# Identify most distinctive features for each cluster\n",
        "print(\"\\n=== MOST DISTINCTIVE FEATURES PER CLUSTER ===\")\n",
        "for cluster_id in range(K):\n",
        "    cluster_data = cluster_means.iloc[cluster_id]\n",
        "    overall_mean = clusters_df.drop('cluster_label', axis=1).mean()\n",
        "    \n",
        "    # Calculate deviations from overall mean\n",
        "    deviations = (cluster_data - overall_mean).abs()\n",
        "    top_features = deviations.nlargest(5).index.tolist()  # Top 5 features\n",
        "    \n",
        "    print(f\"\\nCluster {cluster_id} (n={cluster_counts[cluster_id]}, {cluster_counts[cluster_id]/len(clusters_df)*100:.1f}%):\")\n",
        "    for feature in top_features:\n",
        "        cluster_val = cluster_data[feature]\n",
        "        overall_val = overall_mean[feature]\n",
        "        deviation = cluster_val - overall_val\n",
        "        deviation_pct = (deviation / overall_val) * 100 if overall_val != 0 else 0\n",
        "        print(f\"  - {feature}: {cluster_val:.3f} (deviation: {deviation:+.3f}, {deviation_pct:+.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_file = 'stock_data_clustered_k3_zscore.csv'\n",
        "clusters_df.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"=== RESULTS SAVED ===\")\n",
        "print(f\"Clustered dataset saved as: {output_file}\")\n",
        "print(f\"Dataset shape: {clusters_df.shape}\")\n",
        "print(f\"Features: {len(clusters_df.columns)-1} (plus cluster_label)\")\n",
        "print(f\"\\nModel Summary:\")\n",
        "print(f\"- Number of clusters: {K}\")\n",
        "print(f\"- Silhouette score: {silhouette_avg:.4f}\")\n",
        "print(f\"- Best seed: {best_seed}\")\n",
        "print(f\"- Inertia: {inertia:.2f}\")\n",
        "print(f\"- Standardization: Z-score (mean=0, std=1)\")\n",
        "print(f\"- Cluster balance ratio: {balance_ratio:.3f}\")\n",
        "\n",
        "# Comparison with k=9 model\n",
        "print(f\"\\n=== COMPARISON WITH K=9 MODEL ===\")\n",
        "print(f\"Advantages of k=3:\")\n",
        "print(f\"- Simpler interpretation and business insights\")\n",
        "print(f\"- More distinct cluster characteristics\")\n",
        "print(f\"- Better for strategic decision making\")\n",
        "print(f\"- Reduced over-segmentation\")\n",
        "print(f\"- Easier to communicate results to stakeholders\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Conclusions\n",
        "\n",
        "### Z-Score Standardization Benefits\n",
        "- **Equal Feature Weight**: All features contribute equally to clustering (mean=0, std=1)\n",
        "- **Scale Independence**: Features with different scales don't dominate the clustering\n",
        "- **Interpretable Centroids**: Cluster centroids show how many standard deviations above/below average each feature is\n",
        "- **Robust Clustering**: Less sensitive to outliers and extreme values\n",
        "\n",
        "### Model Performance\n",
        "- **Silhouette Score**: Measures cluster quality and separation\n",
        "- **Inertia**: Within-cluster sum of squares (lower is better)\n",
        "- **Convergence**: Algorithm reached optimal solution\n",
        "- **Cluster Balance**: Distribution of samples across clusters\n",
        "\n",
        "### Cluster Interpretation (k=3)\n",
        "The three clusters likely represent:\n",
        "\n",
        "- **Cluster 0**: [Describe based on distinctive features - e.g., \"Conservative/Bearish\"]\n",
        "- **Cluster 1**: [Describe based on distinctive features - e.g., \"Moderate/Neutral\"] \n",
        "- **Cluster 2**: [Describe based on distinctive features - e.g., \"Aggressive/Bullish\"]\n",
        "\n",
        "### Business Insights\n",
        "The 3-cluster model provides clear strategic insights:\n",
        "\n",
        "- **Portfolio Segmentation**: Three distinct investment strategy categories\n",
        "- **Risk Management**: Clear risk profiles for each cluster\n",
        "- **Recommendation Strategy**: Tailored approaches for each cluster type\n",
        "- **Market Positioning**: Understanding different analyst perspectives\n",
        "\n",
        "### Advantages of k=3 vs k=9\n",
        "- **Simplicity**: Easier to understand and communicate\n",
        "- **Strategic Value**: Better for high-level business decisions\n",
        "- **Interpretability**: Clearer cluster characteristics\n",
        "- **Actionability**: More practical for implementation\n",
        "- **Reduced Noise**: Less over-segmentation of similar patterns\n",
        "\n",
        "### Next Steps\n",
        "1. **Cluster Naming**: Assign meaningful business names to each cluster\n",
        "2. **Validation**: Test cluster stability with different samples\n",
        "3. **Feature Importance**: Identify which features drive cluster separation\n",
        "4. **Temporal Analysis**: Examine how clusters change over time\n",
        "5. **Business Rules**: Develop rules for assigning new recommendations to clusters\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
