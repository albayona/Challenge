{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Clustering Model (k=9)\n",
    "## Stock Data Clustering Analysis\n",
    "\n",
    "This notebook implements K-means clustering with k=9 clusters based on the methodology from Taller_4.ipynb.\n",
    "We'll analyze the independent features dataset to identify distinct stock recommendation patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Sklearn imports\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "print(\"Libraries imported successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the independent features dataset\n",
    "df = pd.read_csv('stock_data_independent_features.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Features: {list(df.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality check and cleaning\n",
    "print(\"=== DATA QUALITY CHECK ===\")\n",
    "print(f\"Infinite values: {np.isinf(df).sum().sum()}\")\n",
    "print(f\"NaN values: {df.isnull().sum().sum()}\")\n",
    "\n",
    "# Check for extremely large values\n",
    "large_values = (np.abs(df.select_dtypes(include=[np.number])) > 1e10).sum().sum()\n",
    "print(f\"Values > 1e10: {large_values}\")\n",
    "\n",
    "# Clean the data\n",
    "print(\"\\n=== CLEANING DATA ===\")\n",
    "# Replace infinite values with NaN\n",
    "df_clean = df.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Drop rows with any NaN values\n",
    "df_clean = df_clean.dropna()\n",
    "\n",
    "# Clip extremely large values to reasonable range\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
    "for col in numeric_cols:\n",
    "    df_clean[col] = df_clean[col].clip(lower=-1e6, upper=1e6)\n",
    "\n",
    "print(f\"Cleaned dataset shape: {df_clean.shape}\")\n",
    "print(f\"Features: {list(df_clean.columns)}\")\n",
    "\n",
    "# Verify no problematic values remain\n",
    "print(f\"\\nAfter cleaning:\")\n",
    "print(f\"Infinite values: {np.isinf(df_clean).sum().sum()}\")\n",
    "print(f\"NaN values: {df_clean.isnull().sum().sum()}\")\n",
    "print(f\"Values > 1e6: {(np.abs(df_clean.select_dtypes(include=[np.number])) > 1e6).sum().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data (following Taller_4 methodology)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(df_clean)\n",
    "scaled_df = pd.DataFrame(scaled_data, columns=df_clean.columns)\n",
    "\n",
    "print(f\"Standardized data shape: {scaled_data.shape}\")\n",
    "print(f\"\\nStandardized data statistics:\")\n",
    "print(scaled_df.describe().round(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration parameters (following Taller_4 methodology)\n",
    "K = 6  # Number of clusters (from elbow method)\n",
    "N_INIT = 10  # Number of initializations\n",
    "SEED = 42  # Random state for reproducibility\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"- Number of clusters (k): {K}\")\n",
    "print(f\"- Number of initializations: {N_INIT}\")\n",
    "print(f\"- Random seed: {SEED}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for best seed (from Taller_4)\n",
    "def search_seed(seeds, data, k, n_init):\n",
    "    best_seed = None\n",
    "    best_score = -1\n",
    "    \n",
    "    for seed in seeds:\n",
    "        km_model = KMeans(\n",
    "            n_clusters=k, init='k-means++', random_state=seed, n_init=n_init)\n",
    "        y_predicted = km_model.fit_predict(data)\n",
    "\n",
    "        silhouette_avg = silhouette_score(data, y_predicted)\n",
    "        \n",
    "        if silhouette_avg > best_score:\n",
    "            best_score = silhouette_avg\n",
    "            best_seed = seed\n",
    "            \n",
    "        print(f\"Seed {seed}: Silhouette Score = {silhouette_avg:.4f}\")\n",
    "    \n",
    "    return best_seed, best_score\n",
    "\n",
    "# Search for best seed\n",
    "seeds_to_test = [0, 42, 123, 456, 789]\n",
    "best_seed, best_score = search_seed(seeds_to_test, scaled_data, K, N_INIT)\n",
    "\n",
    "print(f\"\\n=== BEST SEED RESULTS ===\")\n",
    "print(f\"Best seed: {best_seed}\")\n",
    "print(f\"Best silhouette score: {best_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final K-means model with best seed\n",
    "print(f\"Training K-means model with k={K} and seed={best_seed}...\")\n",
    "\n",
    "final_km = KMeans(n_clusters=K, init='k-means++', random_state=best_seed, n_init=N_INIT)\n",
    "final_km.fit(scaled_data)\n",
    "y_predicted = final_km.predict(scaled_data)\n",
    "\n",
    "# Calculate final metrics\n",
    "silhouette_avg = silhouette_score(scaled_data, y_predicted)\n",
    "inertia = final_km.inertia_\n",
    "\n",
    "print(f\"\\n=== MODEL RESULTS ===\")\n",
    "print(f\"Silhouette Score: {silhouette_avg:.4f}\")\n",
    "print(f\"Inertia: {inertia:.2f}\")\n",
    "print(f\"Number of iterations: {final_km.n_iter_}\")\n",
    "print(f\"Converged: {final_km.n_iter_ < final_km.max_iter}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print cluster centroids (from Taller_4)\n",
    "def print_clusters_centroids(model, feature_names, k):\n",
    "    info = {\n",
    "        \"features\": feature_names \n",
    "    }\n",
    "\n",
    "    for n in range(0, k):\n",
    "        info[str(n)] = model.cluster_centers_[n]\n",
    "\n",
    "    print(tabulate(info, headers=\"keys\", tablefmt=\"fancy_grid\"))\n",
    "    print(f\"\\nCluster distribution:\")\n",
    "    print(Counter(model.labels_))\n",
    "\n",
    "# Print cluster centroids and distribution\n",
    "print(\"=== CLUSTER CENTROIDS ===\")\n",
    "print_clusters_centroids(final_km, scaler.feature_names_in_, K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create clusters dataframe for analysis\n",
    "clusters_df = df_clean.copy()\n",
    "clusters_df['cluster_label'] = final_km.labels_\n",
    "\n",
    "print(f\"Clusters dataframe shape: {clusters_df.shape}\")\n",
    "print(f\"\\nCluster distribution:\")\n",
    "cluster_counts = clusters_df['cluster_label'].value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    percentage = (count / len(clusters_df)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {count} samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nFirst few rows with cluster labels:\")\n",
    "clusters_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot clusters (from Taller_4)\n",
    "def plot_clusters(model, data):\n",
    "    plt.figure(figsize=(20, 10))\n",
    "\n",
    "    label = model.labels_\n",
    "    u_labels = np.unique(label)\n",
    "\n",
    "    # Getting the Centroids\n",
    "    centroids = model.cluster_centers_\n",
    "    \n",
    "    # Plotting the results:\n",
    "    for i in u_labels:\n",
    "        plt.scatter(data[label == i, 0], data[label == i, 1], \n",
    "                   label=f'Cluster {i}', alpha=0.7, s=50)\n",
    "    \n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], \n",
    "               s=200, c='red', marker='x', label='Centroids', linewidths=3)\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.title(f'K-Means Clustering (k={K})')\n",
    "    plt.xlabel('First Principal Component')\n",
    "    plt.ylabel('Second Principal Component')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "\n",
    "# Note: Since we have many features, we'll use PCA for visualization\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA for 2D visualization\n",
    "pca = PCA(n_components=2)\n",
    "data_pca = pca.fit_transform(scaled_data)\n",
    "\n",
    "print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Total explained variance: {pca.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "# Plot clusters in 2D PCA space\n",
    "plot_clusters(final_km, data_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to draw polar chart (from Taller_4)\n",
    "def draw_polar_chart(clusters):\n",
    "    polar = clusters.groupby(\"cluster_label\").mean().reset_index()\n",
    "    polar = pd.melt(polar, id_vars=[\"cluster_label\"])\n",
    "\n",
    "    fig = px.line_polar(polar, r=\"value\", theta=\"variable\", color=\"cluster_label\", \n",
    "                       line_close=True, height=800, width=1400,\n",
    "                       title=\"Cluster Characteristics - Polar Chart\")\n",
    "    fig.show()\n",
    "\n",
    "# Create polar chart to visualize cluster characteristics\n",
    "print(\"Generating polar chart to visualize cluster characteristics...\")\n",
    "draw_polar_chart(clusters_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot clusters pie chart (from Taller_4)\n",
    "def plot_clusters_pie(clusters):\n",
    "    pie = clusters.groupby('cluster_label').size().reset_index()\n",
    "    pie.columns = ['cluster_label', 'value']\n",
    "    fig = px.pie(pie, values='value', names='cluster_label', \n",
    "                title=\"Cluster Distribution\")\n",
    "    fig.show()\n",
    "\n",
    "# Create pie chart for cluster distribution\n",
    "print(\"Generating pie chart for cluster distribution...\")\n",
    "plot_clusters_pie(clusters_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed cluster analysis\n",
    "print(\"=== DETAILED CLUSTER ANALYSIS ===\")\n",
    "\n",
    "# Calculate mean values for each cluster\n",
    "cluster_means = clusters_df.groupby('cluster_label').mean()\n",
    "\n",
    "print(\"\\nCluster characteristics (mean values):\")\n",
    "print(cluster_means.round(3))\n",
    "\n",
    "# Identify most distinctive features for each cluster\n",
    "print(\"\\n=== MOST DISTINCTIVE FEATURES PER CLUSTER ===\")\n",
    "for cluster_id in range(K):\n",
    "    cluster_data = cluster_means.iloc[cluster_id]\n",
    "    overall_mean = clusters_df.drop('cluster_label', axis=1).mean()\n",
    "    \n",
    "    # Calculate deviations from overall mean\n",
    "    deviations = (cluster_data - overall_mean).abs()\n",
    "    top_features = deviations.nlargest(3).index.tolist()\n",
    "    \n",
    "    print(f\"\\nCluster {cluster_id} (n={cluster_counts[cluster_id]}):\")\n",
    "    for feature in top_features:\n",
    "        cluster_val = cluster_data[feature]\n",
    "        overall_val = overall_mean[feature]\n",
    "        deviation = cluster_val - overall_val\n",
    "        print(f\"  - {feature}: {cluster_val:.3f} (deviation: {deviation:+.3f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "output_file = 'stock_data_clustered_k9.csv'\n",
    "clusters_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"=== RESULTS SAVED ===\")\n",
    "print(f\"Clustered dataset saved as: {output_file}\")\n",
    "print(f\"Dataset shape: {clusters_df.shape}\")\n",
    "print(f\"Features: {len(clusters_df.columns)-1} (plus cluster_label)\")\n",
    "print(f\"\\nModel Summary:\")\n",
    "print(f\"- Number of clusters: {K}\")\n",
    "print(f\"- Silhouette score: {silhouette_avg:.4f}\")\n",
    "print(f\"- Best seed: {best_seed}\")\n",
    "print(f\"- Inertia: {inertia:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "### Model Performance\n",
    "- **Silhouette Score**: Measures how similar an object is to its own cluster compared to other clusters (higher is better)\n",
    "- **Inertia**: Sum of squared distances of samples to their closest cluster center (lower is better)\n",
    "- **Convergence**: Whether the algorithm converged within the maximum iterations\n",
    "\n",
    "### Cluster Interpretation\n",
    "The polar chart and cluster centroids table help identify the characteristics of each cluster:\n",
    "- **Cluster 0**: [Describe based on distinctive features]\n",
    "- **Cluster 1**: [Describe based on distinctive features]\n",
    "- **Cluster 2**: [Describe based on distinctive features]\n",
    "- **Cluster 3**: [Describe based on distinctive features]\n",
    "- **Cluster 4**: [Describe based on distinctive features]\n",
    "- **Cluster 5**: [Describe based on distinctive features]\n",
    "- **Cluster 6**: [Describe based on distinctive features]\n",
    "- **Cluster 7**: [Describe based on distinctive features]\n",
    "- **Cluster 8**: [Describe based on distinctive features]\n",
    "\n",
    "### Business Insights\n",
    "The clustering analysis reveals distinct patterns in stock recommendation data that can be used for:\n",
    "- **Portfolio segmentation**: Different clusters may represent different investment strategies\n",
    "- **Risk assessment**: Clusters with similar characteristics may have similar risk profiles\n",
    "- **Recommendation optimization**: Understanding cluster patterns can improve recommendation algorithms\n",
    "\n",
    "### Next Steps\n",
    "1. **Cluster validation**: Use additional metrics like Davies-Bouldin index\n",
    "2. **Feature importance**: Analyze which features contribute most to cluster separation\n",
    "3. **Temporal analysis**: Examine how clusters change over time\n",
    "4. **Predictive modeling**: Use cluster labels as features for classification tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
